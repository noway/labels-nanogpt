big model with labels, subtraction:

eval_type ('nshot_cot',)
correct_count (14,)
all_count (55,)

eval_type ('nshot_direct',)
correct_count (14,)
all_count (55,)

eval_type ('0shot_cot',)
correct_count (16,)
all_count (55,)

eval_type ('0shot_direct',)
correct_count (21,)
all_count (55,)

big model with labels (word filler fixed), subtraction:

eval_type ('nshot_cot',)
correct_count (16,)
all_count (55,)

eval_type ('nshot_direct',)
correct_count (12,)
all_count (55,)

eval_type ('0shot_cot',)
correct_count (16,)
all_count (55,)

eval_type ('0shot_direct',)
correct_count (20,)
all_count (55,)

big model with no labels, subtraction:

eval_type ('nshot_cot',)
correct_count (4,)
all_count (55,)

eval_type ('nshot_direct',)
correct_count (7,)
all_count (55,)

eval_type ('0shot_cot',)
correct_count (7,)
all_count (55,)

eval_type ('0shot_direct',)
correct_count (9,)
all_count (55,)

big model with label embedding, subtraction:

eval_type ('nshot_cot',)
correct_count (13,)
all_count (55,)

eval_type ('nshot_direct',)
correct_count (10,)
all_count (55,)

eval_type ('0shot_cot',)
correct_count (8,)
all_count (55,)

eval_type ('0shot_direct',)
correct_count (15,)
all_count (55,)

interpretation:
- label embedding improves intelligence, but having a separate token for the label is better. having a separate token can be compared with halving the context window and doubling the number of parameters