generate practice book [done]
generate Workbooks [done]
generate Teacher's Edition Textbooks
generate interactive activities (agentic)
generate educational games (agentic)
generate project-based learning activities
generate advanced problem sets

*todo*
- consistent formatting in textbooks/workbooks/etc
- actually review textbooks/worksbooks/etc
- make sure textbook answer key is ALWAYS provided
- remove encourangements [no]
- look for "the student" or "student"
- might have to regenerate the review chapter or delete
- should learn on the dataset sequentially
- 10% from the end of the dataset is actually not very representative of the whole dataset.
- render md -> html -> elinks (would this work for highlighting bold?) (maybe convert terminal seqs back to md-light?) [done]
- render latex somehow? [done]
- start token for every section/chapter [done]
- look for "the actual" and "will vary" or "depend" - replace them with an "unsolved problem" or something [done]
- :apple: - might need to replace with proper emoji? [done]
- look for "next exciting chapter" strings somehow [done]
- look for "(Note" (case insensitive) [done]
- "Extra Challenge" is not a thing (it actually is) [done]
- "hypothetical book" - remove [done]
- look for "[Insert" [done]
- look for "*Note" [done]
- look for "(provide [done]
- look for "*Note" again [done]
- look for "Please note" [done]
- look for "will vary" [done]
- · should be × [no]
- replace ¼ with 1/4 [done]
- add ruff [done]
- implement abbreviation/hiraginazition idea [no]
- remove pyhyphen (doesn't work that well for English) [done]
- add 'document_start' token to word map [done]
- new tokenization approach: letter map for high freq words, abbr for middle dist words, low freq words as is [no]
- maybe do a word commonality label? like word difficulty in thesaurus
- 'tree of models' - this could be revolutionary. one model to parse morphology. another model on top to parse higher order concepts. than even higher order concepts. etc. etc. etc. every single one has its own tokenization. actually no, just sounds like tokenization in between layers [no]

*sharp edges*
- there's some stuff like environment (classroom) specfic games where answer is random. can't learn much from it
- there should be a mechanism to determine where model struggles to learn